{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Rough Skeleton of Working Draft\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "### Table of Contents:\n",
    "1. Sypnosis\n",
    "<br>\n",
    "    a) Research Question\n",
    "    <br>\n",
    "    b) Overview\n",
    "<p></p>\n",
    "2. Selection of Data \n",
    "<br>\n",
    "    a) Packages Used\n",
    "    <br>\n",
    "    b) Loading and Tidying Data\n",
    "<p></p>\n",
    "3. Training, Validation, and Testing Sets\n",
    "<br>\n",
    "    a) Compartmentalization\n",
    "    <br>\n",
    "    b) Training Data Summaries\n",
    "<p></p>\n",
    "4. Preprocessing \n",
    "<br>\n",
    "    a) Preliminary Data Analysis\n",
    "    <br>\n",
    "    b) Indetification of Class Imbalances\n",
    "    <br>\n",
    "    c) Balancing Decision\n",
    "<p></p>\n",
    "5. Building our Model\n",
    "<br>\n",
    "    a) Overview \n",
    "    <br>\n",
    "    b) Tuning\n",
    "    <br>\n",
    "    c) Accuracy Comparison\n",
    "<p></p>\n",
    "6. Analysis\n",
    "<br>\n",
    "    a) Classifying a New Observation\n",
    "<p></p>\n",
    "7. Results and Discussion\n",
    "<br>\n",
    "    a) Results\n",
    "    <br>\n",
    "    b) Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sypnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Research Question:\n",
    "How can an author increase engagement from users on Facebook and can we predict the success of a post using insights from an author's page?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Overview:\n",
    "The market utility of social media platforms such as Facebook, which are able to generate mass revenues for cosmetic brands, has been an established and exploited advertising strategy in the digital age (Moro et. al, 2016).\n",
    "The goal of this project is to take a predictive analytical approach to determine which type of Facebook post (i.e., photo, video, status, or link) will engage the most internet-user engagement, determined through variables such as likes, post consumptions, and post total reach.\n",
    "The dataset which will be used for this analysis was acquired through an experimental data mining technique which included scraping data from the Facebook page of an internationally renowned cosmetics company on posts made between January 1st and December 31st (Moro et. al., 2016).\n",
    "\n",
    "We will use the variables of the continuous numerical variables of total reaches (Lifetime_Post_Total_Reach) and the number of total interactions (Total_Interactions), and the categorical variable of Facebook post (Type). We chose the two continuous numerical variables of total reach and total interactions as predictors because they are key indicators of success that are interrelated. Although we do not consider factors such as the type of users (influencers, company profiles, regular users,etc.), the variable of total reach must be put in relations with total interactions to offer a valuable insight. That is because total interactions influence Facebook's algorithm that may result in a viral post and thus increase the total reach regardless of the extent of the user's total network (number of friends, affiliated groups,etc.)(Quesenberry & Coolsen, 2018). Therefore, the variables of total reach and total interactions must be paired to fully graps the success of a post as both can create a postivie feedback loop. \n",
    "\n",
    "For the methodology, we will look at the relationship between these variables in a scatter plot graph that will help us to formulate our hypothesis. Then, as we are trying to predict the type of post that will be the most successful, we will use a K-nearest neighbour classification analysis. To do so, we must determine the K value using cross-validation of the training data.Then, we will need to test the accuracy of the classifier with the testing data.\n",
    "\n",
    "We expect to find that posts which include media, such as photos and videos, are more likely to engage users than other posts, such as statuses and links.This is based on the assumption that the former types of posts might be more likely to be shared and thus will have more exposure.It is beneficial for social media platforms to increase user engagement, as this is likely to increase revenue through advertising.Therefore, these findings may be used to choose what type of posts are prioritized to maximize user engagement.\n",
    "\n",
    "These findings may lead to further exploration of how the contents of these posts impact user engagement.\n",
    "This may include the duration of a video, content of an image, length of a status, or details about the contents of a link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selection of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global options\n",
    "\n",
    "Here, we set some global parameters in order to reduce cluttering of output information as well as the visibility of warning messages. The latter is enforced later in the code with suppressMessages()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.matrix.max.rows = 8)\n",
    "options(warn = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) List of Packages Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressMessages(install.packages(\"cowplot\"))\n",
    "suppressMessages(install.packages(\"caTools\"))\n",
    "suppressMessages(install.packages(\"gridExtra\"))\n",
    "suppressMessages(install.packages(\"ggpubr\"))\n",
    "\n",
    "suppressMessages(library(caTools))\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(repr))\n",
    "suppressMessages(library(tidymodels))\n",
    "suppressMessages(library(kknn))\n",
    "suppressMessages(library(MASS))\n",
    "suppressMessages(library(cowplot))\n",
    "suppressMessages(library(ggplot2))\n",
    "suppressMessages(library(gridExtra))\n",
    "suppressMessages(library(grid))\n",
    "suppressMessages(library(ggpubr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Loading and Tidying Data\n",
    "First, we read in our data from a remote GitHub repository.\n",
    "We ensured that all variables were converted to factors (i.e., categorical variables) since this data type agrees well with KNN classification algorithms.\n",
    "Further, since certain functions do not permit manipuation of columns which have a space in their name, we removed all spaces from column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_url <-\n",
    "    \"https://raw.githubusercontent.com/calamari99/Facebook-Post-Predictor/main/data/dataset_Facebook.csv\"\n",
    "\n",
    "facebook <- read_csv2(facebook_url) %>%\n",
    "    suppressMessages()\n",
    "\n",
    "# Replacing specified columns to categorical factors\n",
    "cols <- c(\"Type\", \"Category\", \"Post Month\", \"Paid\", \"Post Weekday\", \"Post Hour\")\n",
    "facebook[cols] <- lapply(facebook[cols], as.factor)\n",
    "\n",
    "# Renaming column headers in order to remove spaces\n",
    "facebook_colname_fix <- facebook\n",
    "\n",
    "facebook_col_name_vec <- gsub(\" \", \"_\", colnames(facebook))\n",
    "colnames(facebook) <- facebook_col_name_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let us select only the variables relevant to our analysis.\n",
    "\n",
    "To specify the best type of post possible and to explore the relationship between the metrics produced by a post and the post type.\n",
    "We determined that the following key performance indicators best describe a post's success:\n",
    "- comments\n",
    "- likes\n",
    "- shares\n",
    "- total interactions (summation of the 3 observations above) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables relevant to our analysis\n",
    "facebook_clean <- dplyr::select(facebook, Type, comment,\n",
    "                                like, share, Total_Interactions,\n",
    "                                Paid, Lifetime_Post_Total_Impressions,\n",
    "                                Lifetime_Post_Total_Reach) %>%\n",
    "                            na.omit(df)\n",
    "\n",
    "# Separate paid and unpaid posts\n",
    "facebook_clean_unpaid <- facebook_clean %>% filter(Paid == 0)\n",
    "facebook_clean_paid <- facebook_clean %>% filter(Paid == 1)\n",
    "\n",
    "# Summary of unpaid posts by type\n",
    "unpaid_summary <- facebook_clean_unpaid %>% group_by(Type) %>% \n",
    "    summarise(unpaid = n()) \n",
    "\n",
    "# Summary of paid posts by type\n",
    "paid_summary <- facebook_clean_paid %>% group_by(Type) %>% \n",
    "    summarise(paid = n())\n",
    "\n",
    "Reduce(dplyr::full_join, list(unpaid_summary, paid_summary)) %>%\n",
    "    suppressMessages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within our 500 data points collected, we have filtered out all observations with NA values and separated our data into paid and unpaid categories due to additive relationships.\n",
    "This allows us to explore the relationship between post type and our defined success metric.\n",
    "Moving forward, this study will only evaluate on media postings without paid advertising.\n",
    "\n",
    "> Note: Social media algorithms that adjust prioritizations between paid and non-paid posts can heavily factor into our metrics received and should be considered in this analysis. To control for this potential source of uncertainty, we haved isolated our data into paid and unpaid categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training, Validation, and Testing Sets \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Compartmentalization\n",
    "We have split our data into training and testing sets in order to reduce bias within our model data and testing data. \n",
    "\n",
    "*Distribution of Training and Testing set*\n",
    "<br>\n",
    "- Testing set will be 20% of data collected\n",
    "<br>\n",
    "- Validation set will be 10% of data collected\n",
    "<br>\n",
    "- Training data set be 70% of data collected\n",
    "\n",
    "*Cross-validation technique*\n",
    "<br>\n",
    "let us split our data into 10 total groups.\n",
    "<br>\n",
    "(~25 points tested, 100 points for training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to approach our training data by creating a 80:20 ratio between testing and training data where the  training set is composed of both the “validation” and “training” set.\n",
    "We have also chosen a 10-fold cross-validation procedure to establish unbiased estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Variables\n",
    "set.seed(99)\n",
    "partitionTrain = 0.8\n",
    "ratioTrainValidation = 7/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 ratio TrainingSet:TestingSet\n",
    "split <- sample.split(facebook_clean$like, SplitRatio = partitionTrain)\n",
    "train_val_data <- subset(facebook_clean, split == TRUE)\n",
    "test_set <- subset(facebook_clean, split == FALSE)\n",
    "\n",
    "split <- sample.split(train_val_data$like, SplitRatio = ratioTrainValidation)\n",
    "train_set <- subset(train_val_data, split == TRUE)\n",
    "val_set <- subset(train_val_data, split == FALSE)\n",
    "\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpaid Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split <- sample.split(facebook_clean_unpaid$like, SplitRatio = partitionTrain)\n",
    "train_val_data_unpaid <- subset(facebook_clean_unpaid, split == TRUE)\n",
    "test_set_unpaid <- subset(facebook_clean_unpaid, split == FALSE)\n",
    "\n",
    "split <- sample.split(train_val_data_unpaid$like, SplitRatio = ratioTrainValidation)\n",
    "train_set_unpaid <- subset(train_val_data_unpaid, split == TRUE)\n",
    "val_set_unpaid <- subset(train_val_data_unpaid, split == FALSE)\n",
    "\n",
    "#glimpse(train_set_unpaid)\n",
    "train_set_unpaid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### b) Training Data Summaries\n",
    "\n",
    "- Number of observations of each type\n",
    "- Mean and Median of key metrics in each post type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary of Unpaid Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_train_unpaid <- train_set_unpaid %>%\n",
    "    group_by(Type) %>%\n",
    "    summarise(\n",
    "        count = n(),\n",
    "        mean_comment = mean(comment), \n",
    "        median_comment = median(comment), \n",
    "        mean_like = mean(like),\n",
    "        median_like = median(like),\n",
    "        mean_Total_Interactions = mean(Total_Interactions),\n",
    "        median_Total_Interactions = median(Total_Interactions),\n",
    "        mean_share = mean(share),\n",
    "        median_share = median(share)) %>%\n",
    "    suppressMessages()\n",
    "\n",
    "summ_train_unpaid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Preliminary Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inform analysis later on, we first conducted a preliminary exploration of our dataset.\n",
    "<p>\n",
    "In order to gain more insight on the relationship between each type of post and the magnitude of its interactions, we started by taking the mean value of each interaction type with respect to each post type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_comment <- summ_train_unpaid$mean_comment\n",
    "mean_like <- summ_train_unpaid$mean_like\n",
    "mean_Total_Interactions <- summ_train_unpaid$mean_Total_Interactions\n",
    "mean_share <- summ_train_unpaid$mean_share\n",
    "type <- summ_train_unpaid$Type\n",
    "\n",
    "test_df <- data.frame(type,\n",
    "                      mean_comment,\n",
    "                      mean_like,\n",
    "                      mean_share,\n",
    "                      mean_Total_Interactions)\n",
    "\n",
    "test_df\n",
    "\n",
    "fb_long <- test_df %>%\n",
    "gather(\"Stat\", \"Value\", -type)\n",
    "\n",
    "fb_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare how many likes, each post receives, we can filter the above table to only show likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_mean_like <- fb_long %>%\n",
    "    filter(Stat == \"mean_like\") %>%\n",
    "    arrange(desc(Value))\n",
    "\n",
    "filter_mean_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above table that videos tend to receive the most likes on average, while links tend to receive the fewest. We can compare these values by plotting them on a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 6) \n",
    "\n",
    "mean_likes_bar <- ggplot(filter_mean_like, aes(x = type, y = Value, fill = type)) +\n",
    "    geom_bar(stat = \"identity\") +\n",
    "    labs(x = \"Post Type\", y = \"Mean Number of Likes\",\n",
    "         title = \"Mean number of likes per post type\") +\n",
    "    theme(text = element_text(size = 20)) \n",
    "mean_likes_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph above that videos tend to receive significantly more likes than other post types. Additionally, photos and status posts receive similar number of likes compared to each other. Links receive significantly fewer likes than all other types of posts.\n",
    "<p>\n",
    "If we repeat this process without filtering for likes, we can compare the engagement per post across all types of engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=10)\n",
    "mean_fb <- ggplot(fb_long, aes(x = type, y = Value, fill = Stat)) +\n",
    "    geom_col(position = \"dodge\") +\n",
    "    labs(x = \"Type of Post\", y = \"Count\", title=\"Mean interactions per post type\") +\n",
    "    scale_fill_discrete(name = \"Stats\",\n",
    "                        labels = c(\"Mean Comment\",\n",
    "                                   \"Mean Like\",\n",
    "                                   \"Mean Share\",\n",
    "                                   \"Mean Total Interactions\"))+\n",
    "    theme(text = element_text(size = 20))\n",
    "mean_fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we can see that the ratio between the amounts of engagement for different types of post are roughtly equal regardless of the type of interaction. Additionally, all types of posts tend to receive very few comments and shares compared to the number of likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpaid_plot <- facebook_clean_unpaid %>% \n",
    "    ggplot(aes(x = Lifetime_Post_Total_Reach/100,\n",
    "               y = Total_Interactions,\n",
    "               shape = Type,\n",
    "               color = Type,\n",
    "               fill = Type)) +\n",
    "    geom_point(alpha=0.6, size=4) +\n",
    "    labs(x = \"Total reach (hundreds)\", y = \"Total interactions\",\n",
    "         title = \"Total reach vs total interactions\", group = \"Type\")+\n",
    "    scale_y_continuous(limits = c(0,900)) +\n",
    "    scale_x_continuous(limits = c(0,500)) +\n",
    "    scale_shape_manual(values = c(21,22,23,24)) +\n",
    "    scale_size_manual(values=c(1,6,7,9)) +\n",
    "    theme_minimal() +\n",
    "    theme(text = element_text(size = 20))\n",
    "    options(repr.plot.width =14, repr.plot.height = 8)\n",
    "unpaid_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Identification of Class Imbalances\n",
    "We want to be able to identify possible class imbalances as the KNN-classification model is a lazy learning algorithm.\n",
    "Thus we need to ensure that our data set is balanced.\n",
    "We start by reviewing summary statistics and quickly visualizing the distribution of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width = 25, repr.plot.height = 15)\n",
    "\n",
    "test_unpaid_hist_1 <- train_set_unpaid %>%\n",
    "    ggplot(aes(fill=Type, x=Lifetime_Post_Total_Reach/100))+\n",
    "    geom_histogram(binwidth = 20,\n",
    "                   center = 1,\n",
    "                   boundary = NULL,\n",
    "                   alpha = 0.7,\n",
    "                   position = position_stack(vjust = 0, reverse=FALSE)) +\n",
    "    labs(x = \"Total reach (hundreds)\", y = \"Number of posts\",\n",
    "         title = \"Total reach per type of post\", fill=\"Type of post\") +\n",
    "    scale_x_continuous(limits = c(0, 600)) +\n",
    "    scale_fill_manual(values = c(\"#eb1515\", \"#15eb15\", \"#1515eb\", \"#eb8015\")) +\n",
    "    theme(text = element_text(size = 20), plot.margin = unit(c(3,3,3,3), \"lines\"))\n",
    "\n",
    "test_unpaid_hist_2 <- train_set_unpaid %>%\n",
    "    ggplot(aes(x=Total_Interactions, fill=Type))+\n",
    "    geom_histogram(binwidth = 20,\n",
    "                   alpha = 0.7,\n",
    "                   position = position_stack(vjust=0, reverse=FALSE)) +\n",
    "    labs(x = \"Total interactions\", y = \"Number of posts\",\n",
    "         title = \"Total interactions per type of post\", fill = \"Type of post\") +\n",
    "    scale_fill_manual(values = c(\"#eb1515\", \"#15eb15\", \"#1515eb\", \"#eb8015\")) +\n",
    "    scale_x_continuous(limits = c(0, 600)) +\n",
    "    theme(text = element_text(size = 20), plot.margin = unit(c(3,3,3,3), \"lines\"))\n",
    "\n",
    "test_val_hist_1 <- val_set %>%\n",
    "    ggplot(aes(fill=Type, x=Lifetime_Post_Total_Reach/100))+\n",
    "    geom_histogram(binwidth = 20,\n",
    "                   center = 1,\n",
    "                   boundary = NULL,\n",
    "                   alpha = 0.7,\n",
    "                   position = position_stack(vjust = 0, reverse = FALSE)) +\n",
    "    labs(x = \"Total reach (hundreds)\", y = \"Number of posts\",\n",
    "         title = \"Total reach per type of post\", fill = \"Type of post\") +\n",
    "    scale_x_continuous(limits = c(0, 600)) +\n",
    "    scale_fill_manual(values = c(\"#eb1515\", \"#15eb15\", \"#1515eb\", \"#eb8015\")) +\n",
    "    theme(text = element_text(size = 20), plot.margin = unit(c(3,3,3,3), \"lines\"))\n",
    "\n",
    "test_val_hist_2 <- val_set %>%\n",
    "    ggplot(aes(x=Total_Interactions, fill=Type))+\n",
    "    geom_histogram(binwidth = 20,\n",
    "                   alpha = 0.7,\n",
    "                   position = position_stack(vjust = 0, reverse = FALSE)) +\n",
    "    labs(x = \"Total interactions\", y = \"Number of posts\",\n",
    "         title = \"Number of Posts vs Total Interactions\", fill = \"Type of post\") +\n",
    "    scale_fill_manual(values = c(\"#eb1515\", \"#15eb15\", \"#1515eb\", \"#eb8015\")) +\n",
    "    scale_x_continuous(limits = c(0, 600)) +\n",
    "    theme(text = element_text(size = 20), plot.margin = unit(c(3,3,3,3), \"lines\"))\n",
    "\n",
    "\n",
    "grid.arrange(test_unpaid_hist_1, test_val_hist_1, test_unpaid_hist_2, test_val_hist_2, ncol=2,\n",
    "             top = textGrob(expression(bold(\"Distributions: Training (Left), Validation (Right)\")),\n",
    "                            x =0, hjust=0, gp=gpar(fontsize = 24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing Decision\n",
    "We see the distribution of the type of posts is not equal so we should consider balancing.\n",
    "However, this introduces potential complications in further parts of our analysis, mainly the cross validation step.\n",
    "We find that balancing our data in this part of our analysis results in overestimated accuracies for our cross validation model later on.\n",
    "Additionally, we are hestitant to balance training set because this alternation is not reflected in our testing set, which can lead to more uncertainty.\n",
    "Because of these factors, we chose to leave our data unbalanced.\n",
    "We believe this will lead to less biased results when using our training data set further in our report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Building our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Overview\n",
    "We use the original training data into our tuning selection process.\n",
    "Then by scaling the data and following the tidymodel recipes workflow, we collect the results from various values of k.\n",
    "Our base value of k is set to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 3) %>%\n",
    "      set_engine(\"kknn\") %>%\n",
    "      set_mode(\"classification\")\n",
    "\n",
    "unpaid_recipe <- recipe(Type ~ \n",
    "                        Lifetime_Post_Total_Reach + Total_Interactions,\n",
    "                        data = train_set_unpaid) %>%\n",
    "                    step_scale(all_predictors()) %>%\n",
    "                    step_center(all_predictors())\n",
    "\n",
    "unpaid_fit <- workflow() %>%\n",
    "    add_recipe(unpaid_recipe) %>%\n",
    "    add_model(knn_spec) %>%\n",
    "    fit(data = train_set_unpaid)\n",
    "\n",
    "unpaid_val_predicted <- predict(unpaid_fit, val_set_unpaid) %>%\n",
    "    bind_cols(val_set_unpaid)\n",
    "\n",
    "unpaid_prediction_accuracy <- unpaid_val_predicted %>%\n",
    "    metrics(truth = Type, estimate = .pred_class)\n",
    "    \n",
    "unpaid_prediction_accuracy\n",
    "\n",
    "unpaid_val_predicted %>%\n",
    "    conf_mat(truth = Type, estimate = .pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-tuned model accuracy against our validation set was roughly 83%. The low accuracy is connected with the collection of observations within the validation data set itself. Our validation set which was sampled at random had only included observations from 2 types: Photo and Status. Upon running our model against this sample population, the collection of predictions included observations from the Link category as well thus creating inaccuracy in our model. In the following steps, we will hopefully tune and increase our base accuracy while accounting for the small sample of the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Tuning our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will perform the cross validation technique with 10 folds to account for randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(99)\n",
    "unpaid_vfold <- vfold_cv(train_set_unpaid, v = 10, strata = Type)\n",
    "\n",
    "unpaid_fit_v2 <- workflow() %>%\n",
    "    add_recipe(unpaid_recipe) %>%\n",
    "    add_model(knn_spec) %>%\n",
    "    fit_resamples(resamples = unpaid_vfold) %>% collect_metrics() %>%\n",
    "    suppressMessages()\n",
    "\n",
    "unpaid_fit_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through a 10 fold cross-validation method, the accuracy of our model averages to approximately 87%. Given a standard error of roughly 0.01, our estimated true average accuracy falls between $[0.86, 0.88]$. The increase in accuracy can be explained through the choice of data used in the \"vfold()\" function. The validation set created from the **vfold()** function splits our training data into subsections while the prediction model compares the validation set against the predicted values. Thus with a better holistic representation of our sample, our accuracy increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next we will perform a parameterization selection method to select a better value for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tune <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\n",
    "    set_engine(\"kknn\") %>%\n",
    "    set_mode(\"classification\") \n",
    "\n",
    "knn_results <- workflow() %>%\n",
    "    add_recipe(unpaid_recipe) %>%\n",
    "    add_model(knn_tune) %>%\n",
    "    tune_grid(resamples = unpaid_vfold, grid = 10) %>% \n",
    "    collect_metrics() %>%\n",
    "    suppressMessages()\n",
    "\n",
    "accuracies <- knn_results %>% \n",
    "    filter(.metric == \"accuracy\")\n",
    "\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then using our collected metrics, we can visualize our accuracies to refine our value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=15, repr.plot.height=8)\n",
    "\n",
    "accuracy_versus_k <- ggplot(accuracies, aes(x = neighbors, y = mean))+\n",
    "    geom_point() +\n",
    "    geom_line() +\n",
    "    labs(x = \"Neighbors\", y = \"Accuracy Estimate\", title = \"K-NN Classification Accuracy by Neighbors\") +\n",
    "    scale_x_continuous(breaks = seq(0, 16, by = 2)) +  # adjusting the x-axis\n",
    "    scale_y_continuous(limits = c(0.8, 1.0)) + # adjusting the y-axis\n",
    "    theme(text = element_text(size = 20))\n",
    "\n",
    "accuracy_versus_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_accurate_k <- knn_results %>% filter(.metric == \"accuracy\") %>% arrange(desc(mean)) %>% slice(1)\n",
    "most_accurate_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization suggests that $k=9$ averages the highest accuracy of ~88% from our 10 cross validation sets.\n",
    "We edit our model specification to take $k=9$ instead of $k=3$ as follows.\n",
    "After doing so, we can compare the accuracy of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same recipe, change spec\n",
    "\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 9) %>%\n",
    "    set_engine(\"kknn\") %>%\n",
    "    set_mode(\"classification\")\n",
    "\n",
    "unpaid_fit_tuned <- workflow() %>%\n",
    "    add_recipe(unpaid_recipe) %>%\n",
    "    add_model(knn_spec) %>%\n",
    "    fit(data = train_set_unpaid)\n",
    "\n",
    "unpaid_val_predicted_tuned <- predict(unpaid_fit_tuned, val_set_unpaid) %>%\n",
    "    bind_cols(val_set_unpaid)\n",
    "\n",
    "unpaid_prediction_accuracy_tuned <- unpaid_val_predicted_tuned %>%\n",
    "    metrics(truth = Type, estimate = .pred_class) %>% filter(.metric == \"accuracy\")\n",
    "\n",
    "model_improvement <- unpaid_prediction_accuracy_tuned$.estimate - unpaid_prediction_accuracy$.estimate\n",
    "\n",
    "unpaid_prediction_accuracy_tuned\n",
    "print(model_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Classifying a New Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obs_total_interactions <- 250\n",
    "new_obs_total_reach <- 200\n",
    "\n",
    "fb_recipe <- recipe(Type ~\n",
    "                    Total_Interactions + Lifetime_Post_Total_Reach,\n",
    "                    data=facebook_clean_unpaid) %>%\n",
    "  step_scale(all_predictors()) %>%\n",
    "  step_center(all_predictors()) %>%\n",
    "  prep()\n",
    "\n",
    "scaled_fb_unpaid <- bake(fb_recipe, facebook_clean_unpaid)\n",
    "\n",
    "scaled_fb_unpaid %>%\n",
    "  dplyr::select(Total_Interactions, Lifetime_Post_Total_Reach, Type) %>%\n",
    "  mutate(dist_from_new = sqrt((Total_Interactions - new_obs_total_interactions)^2 +\n",
    "                              (Lifetime_Post_Total_Reach - new_obs_total_reach)^2)) %>%\n",
    "  arrange(dist_from_new) %>%\n",
    "  slice(1:9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reach_grid <- seq(min(facebook_clean_unpaid$Lifetime_Post_Total_Reach),\n",
    "                  max(facebook_clean_unpaid$Lifetime_Post_Total_Reach),\n",
    "                  length.out = 100)\n",
    "\n",
    "interactions_grid <- seq(min(facebook_clean_unpaid$Total_Interactions),\n",
    "                         max(facebook_clean_unpaid$Total_Interactions),\n",
    "                         length.out = 100)\n",
    "\n",
    "asgrid <- as_tibble(expand.grid(Lifetime_Post_Total_Reach=reach_grid,\n",
    "                                Total_Interactions=interactions_grid))\n",
    "\n",
    "knnPredGrid <- predict(unpaid_fit, asgrid)\n",
    "\n",
    "prediction_table <- bind_cols(knnPredGrid, asgrid) %>% rename(Type = .pred_class)\n",
    "\n",
    "\n",
    "\n",
    "pred_plot <-\n",
    "    ggplot()+\n",
    "    geom_point(data = facebook_clean_unpaid, \n",
    "               mapping = aes(x = Lifetime_Post_Total_Reach/100,\n",
    "               y = Total_Interactions,\n",
    "               color = Type), \n",
    "               alpha=1, size=2) +\n",
    "    geom_point(data = prediction_table, \n",
    "               mapping = aes(x = Lifetime_Post_Total_Reach/100,\n",
    "               y = Total_Interactions,\n",
    "               color = Type),\n",
    "               alpha=0.09, size=18)+\n",
    "    geom_point(aes(x=200, y=250, size=3))+\n",
    "    geom_segment(aes(x=200, y=250, xend=238, yend=2), colour=\"red\")+\n",
    "    annotate(\"text\", x = 200, y =  260, label = \"New post\", size=4)+\n",
    "    labs(x = \"Total reach (hundreds)\", y = \"Total interactions\",\n",
    "               title = \"Prediction Possibilities\")+\n",
    "    scale_y_continuous(limits = c(0,400)) +\n",
    "    scale_x_continuous(limits = c(0,400)) +\n",
    "    theme(text=element_text(size=20))\n",
    "\n",
    "pred_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A visualization for the prediction possibilities\n",
    "options(repr.plot.width =14, repr.plot.height = 8) \n",
    "\n",
    "wkflw_plot <-\n",
    "    ggplot()+\n",
    "    geom_point(data = facebook_clean_unpaid, \n",
    "               mapping = aes(x = Lifetime_Post_Total_Reach/100,\n",
    "               y = Total_Interactions,\n",
    "               color = Type), \n",
    "               alpha=1, size=2) +\n",
    "    geom_point(data = prediction_table, \n",
    "               mapping = aes(x = Lifetime_Post_Total_Reach/100,\n",
    "               y = Total_Interactions,\n",
    "               color = Type),\n",
    "               alpha=0.09, size=6.)+\n",
    "    labs(x = \"Total reach (hundreds)\", y = \"Total interactions\",\n",
    "               title = \"Prediction Possibilities\")+\n",
    "    scale_y_continuous(limits = c(0,900)) +\n",
    "    scale_x_continuous(limits = c(0,1500)) +\n",
    "    theme(text=element_text(size=20))\n",
    "\n",
    "wkflw_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Results\n",
    "\n",
    "After changing our model spec from $k$ = 3 to $k$ = 9, we see that the modifications to our model has increased our accuracy by *8.3%*, (i.e., from *83.33%* to *91.67%*) and thus we will choose the tuned model.\n",
    "\n",
    "A possible reason the unusually high value of $k$ may lie within the validation data set. As previously discussed, the validation set was a poor holistic representation of our sample and to account for false positives, the model produced a higher critical value.\n",
    "\n",
    "Note that throughout our procedure, we chose to use only the unpaid posts as paid posts introduce a new variable that is the amount paid for the advertisement.\n",
    "This variable affects the length of time being promoted and the scope that target specific audiences and thus are more likely to interact with the post.\n",
    "\n",
    "We also used the variables \"total interactions\", which is the aggregation of the variables \"like\", \"share\", and \"comment\", and \"lifetime total reach\" which refers to the number of views, as predictors to predict the type of post.\n",
    "Based on our tuned model, we can predict which type of post will be successfull with an accuracy of 91.67%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Discussion\n",
    "\n",
    "Our model has showed the most accuracy with a $k$ value of 9 with an accuracy of 91.67%.\n",
    "We ran in the problem of an unbalance dataset as videos, links, and status make up 54 posts out of 264 total observations.\n",
    "Our model decided to acknowledge this unbalance and leave it as is for the reason that by balancing it, it drasticaly biased our model to produce a very high accuracy rate (roughly 96%).\n",
    "Considering that without balancing the dataset, the accuracy rate of our model at 91.67% was good enough. \n",
    "\n",
    "As our research question asks which type of posts will be the most sucessful, our model first demonstrated the postive relationshinp between total impressions and total reach by type of posts which allowed to use them as our two main predictors.\n",
    "Our model showed that as total reach increases total impressions (likes, shares, comments) are likely to increase as well.\n",
    "Therefore, the underlying logic of our question is which type of post is likely to have the most reach, but as the type of content, such as videos or links, affects the likeliness of generetaing likes and comments which, in turns, is more likely to be picked up by Facebook's algorithm to then become viral.\n",
    "Thus, it creates a positive feedback loop as viral posts, if made public, increase Facebook's placement of the post in user's newsfeed resulting in a increased reach (Quesennbury & Coolsen, 2018).\n",
    "Therefore, we used both total reach and total impression to predict the success of a Facebook post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography\n",
    "\n",
    "Moro, S., Rita, P., & Vala, B. (2016).\n",
    "\"Predicting social media performance metrics and evaluation of the impact on brand building: A data mining approach\", \n",
    "Journal of Business Research. 69(9), 3341 - 3351. \n",
    "\n",
    "Quesenberry, Keith A. & Coolsen, Michael K.,\"What Makes Facebook Brand Posts Engaging? A Content Analysis of Facebook Brand Post Text That Increases Shares, Likes, and Comments to Influence Organic Viral Reach\", Journal of Current Issues & Research in Advertising, 40(3), 299-244."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
